# 🔐 LLM-Based Code Generation and Vulnerability Analysis

This project explores the security of source code generated by large language models (LLMs). It combines automated code generation using OpenAI’s GPT-3.5 API with a trained machine learning–based vulnerability analyzer. Users can input prompts to generate code and receive real‑time feedback on whether the generated code is secure or vulnerable.

---

## 📖 Project Description

- **Code Generation Module** – Generates source code from natural‑language prompts with GPT‑3.5.  
- **Security Analysis Module** – Detects vulnerabilities in generated or user‑provided code via an ML ensemble trained on the NIST Juliet Suite.  
- **Flask REST API** – Orchestrates generation and analysis endpoints.  
- **React Frontend** – Web UI for prompt entry and result visualization.

---

## 🛠️ Installation Instructions

### Backend (Flask)

```bash
git clone https://github.com/GTU-CyberAI/LLMCode.git
cd LLMCode/src/backend

python -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate
pip install -r requirements.txt

cp .env.example .env            # add your OpenAI key + model paths
python app.py                   # start backend
```


## Frontend (React)
```bash
cd ../../frontend
npm install
npm start
```

---

## 🛠️ Troubleshooting

| Issue                        | Fix                                                         |
|------------------------------|-------------------------------------------------------------|
| `ModuleNotFoundError`        | Activate **venv** & reinstall deps                          |
| Frontend cannot reach backend| Confirm Flask is running and **CORS** is enabled           |
| OpenAI API errors            | Verify `OPENAI_API_KEY` is set in **.env**                 |
| Model‑file load failure      | Ensure model artifacts exist in `src/backend/models/`       |

---

# 📂 FILE_OVERVIEW.md  
_Key files and their purpose in **LLMCode**_

| Path | Purpose | Key Functions / Classes |
|------|---------|-------------------------|
| **src/backend/app.py** | Flask entry‑point that wires every endpoint (`/api/generate-code`, `/api/analyze-existing`, etc.) to the corresponding service. | `create_app()`, `generate_code()`, `analyze_existing()` |
| **src/backend/trained_model_loader.py** | Loads the serialized ML ensemble (pickle / joblib) and exposes a single `predict_vulnerability()` helper used by the API. | `ModelLoader`, `predict_vulnerability()` |
| **src/backend/models/** | Contains trained model artifacts (`*.pkl`, `*.joblib`). These are loaded at runtime—**do not** edit manually. | n/a – binary model files |
| **src/backend/utils/** | Utility helpers shared by generator & analyzer (e.g., text cleaning, code stripping, logging). | `sanitize_code()`, `format_response()` |
| **src/backend/generator/** | Logic for LLM prompts and OpenAI API communication. Separates prompt templates from core Flask code. | `AICodeGenerator.generate()` |
| **src/backend/analyzer/** | Feature extraction + ensemble voting. Splits TF‑IDF vectorizer, tree‑based models, and thresholding. | `FeatureExtractor`, `EnsembleVoter` |
| **src/frontend/** | React SPA. Presents a form for prompts, shows generated code + security report. Communicates with Flask via Axios. | `App.jsx`, `CodeInputForm.jsx`, `ResultCard.jsx` |
| **requirements.txt** | Exact Python packages & versions required to run the backend. Install with `pip install -r requirements.txt`. | n/a |
| **.env.example** | Template for environment variables (e.g., `OPENAI_API_KEY`, model path). Copy to `.env` and fill values. | n/a |
| **README.md** | Main documentation: project intro, installation, usage, troubleshooting, acknowledgements. | n/a |

---

## 🤝 Acknowledgements
- **Course:** *CSE 473 - Network And Information Security*
- **Instructor:** *Dr. Öğr. Üyesi Salih Sarp.*
- **Team Members:** Arife Yurtseven · İlker Yılmaz